{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff8271d5",
   "metadata": {
    "id": "ff8271d5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from scipy.stats import bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcef88d",
   "metadata": {
    "id": "ebcef88d"
   },
   "source": [
    "### Question 1.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecd8be84",
   "metadata": {
    "id": "ecd8be84"
   },
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 7 fields in line 485, saw 8\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m         dfs_test\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 42\u001b[0m         df\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43mskiprows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m         dfs_train\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m#For lying dataset\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:581\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1254\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1252\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1254\u001b[0m     index, columns, col_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1255\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py:225\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 225\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    227\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:805\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:861\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:847\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:1960\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 7 fields in line 485, saw 8\n"
     ]
    }
   ],
   "source": [
    "dfs_test=[]  \n",
    "dfs_train=[]   \n",
    "#For bending1\n",
    "filepath= 'AReM/bending1'\n",
    "files=glob.glob(os.path.join(filepath,\"*.csv\"))\n",
    "\n",
    "for i in files:\n",
    "    if (i==\"AReM/bending1/dataset1.csv\") or (i==\"AReM/bending1/dataset2.csv\"):\n",
    "        \n",
    "        df=pd.read_csv(i,skiprows=4)\n",
    "        dfs_test.append(df)\n",
    "    else:\n",
    "        df=pd.read_csv(i,skiprows=4)\n",
    "        dfs_train.append(df)\n",
    "        \n",
    "#for bending2\n",
    "filepath2= 'AReM/bending2'\n",
    "files=glob.glob(os.path.join(filepath2,\"*.csv\"))\n",
    "\n",
    "for i in files:\n",
    "    if (i==\"AReM/bending2/dataset1.csv\") or (i==\"AReM/bending2/dataset2.csv\"):\n",
    "       \n",
    "        df=pd.read_csv(i,skiprows=4)\n",
    "        dfs_test.append(df)\n",
    "    elif (i==\"AReM/bending2/dataset4.csv\"):\n",
    "        dfs=pd.read_csv(i,sep=' ',skiprows=5,names=[\"# Columns: time\",\"avg_rss12\",\"var_rss12\",\"avg_rss13\",\"var_rss13\",\"avg_rss23\",\"var_rss23\"],index_col=False)\n",
    "        dfs_train.append(dfs)\n",
    "    else:\n",
    "        df=pd.read_csv(i,skiprows=4)\n",
    "        dfs_train.append(df)\n",
    "        \n",
    "#For cycling\n",
    "file_cycle= 'AReM/cycling'\n",
    "files=glob.glob(os.path.join(file_cycle,\"*.csv\"))\n",
    "\n",
    "for i in files:\n",
    "    if ((i==\"cycling/dataset1.csv\") or (i==\"cycling/dataset2.csv\") or(i==\"cycling/dataset3.csv\")):\n",
    "        \n",
    "        df=pd.read_csv(i,skiprows=4)\n",
    "        dfs_test.append(df)\n",
    "    else:\n",
    "        df=pd.read_csv(i,skiprows=4)\n",
    "        dfs_train.append(df)\n",
    "        \n",
    "#For lying dataset\n",
    "file_lying= 'AReM/lying'\n",
    "files=glob.glob(os.path.join(file_lying,\"*.csv\"))\n",
    " \n",
    "\n",
    "for i in files:\n",
    "    if ((i==\"AReM/lying/dataset1.csv\") or (i==\"AReM/lying/dataset2.csv\") or (i==\"AReM/lying/dataset3.csv\")):\n",
    "        \n",
    "        df=pd.read_csv(i,skiprows=4)\n",
    "        dfs_test.append(df)\n",
    "    else:\n",
    "        df=pd.read_csv(i,skiprows=4)\n",
    "        dfs_train.append(df)\n",
    "\n",
    "#For sitting\n",
    "file_sitting= 'AReM/sitting'\n",
    "files=glob.glob(os.path.join(file_sitting,\"*.csv\"))\n",
    " \n",
    "\n",
    "for i in files:\n",
    "    if ((i==\"AReM/sitting/dataset1.csv\") or (i==\"AReM/sitting/dataset2.csv\") or (i==\"AReM/sitting/dataset3.csv\")):\n",
    "        \n",
    "        df=pd.read_csv(i,skiprows=4)\n",
    "        dfs_test.append(df)\n",
    "    else:\n",
    "        df=pd.read_csv(i,skiprows=4)\n",
    "        dfs_train.append(df)\n",
    "\n",
    "#For standing\n",
    "file_standing= 'AReM/standing'\n",
    "files=glob.glob(os.path.join(file_standing,\"*.csv\"))\n",
    " \n",
    "\n",
    "for i in files:\n",
    "    if ((i==\"AReM/standing/dataset1.csv\") or (i==\"AReM/standing/dataset2.csv\") or (i==\"AReM/standing/dataset3.csv\")):\n",
    "        \n",
    "        df=pd.read_csv(i,skiprows=4)\n",
    "        dfs_test.append(df)\n",
    "    else:\n",
    "        df=pd.read_csv(i,skiprows=4)\n",
    "        dfs_train.append(df)\n",
    "        \n",
    "#For walking \n",
    "file_walking= 'AReM/walking'\n",
    "files=glob.glob(os.path.join(file_walking,\"*.csv\"))\n",
    " \n",
    "\n",
    "for i in files:\n",
    "    if ((i==\"AReM/walking/dataset1.csv\") or (i==\"walking/dataset2.csv\") or (i==\"AReM/walking/dataset3.csv\")):\n",
    "        \n",
    "        df=pd.read_csv(i,skiprows=4)\n",
    "        dfs_test.append(df)\n",
    "    else:\n",
    "        df=pd.read_csv(i,skiprows=4)\n",
    "        dfs_train.append(df)             \n",
    "\n",
    "dfs_test=pd.concat(dfs_test,axis=0,ignore_index=True)\n",
    "dfs_train=pd.concat(dfs_train,axis=0,ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abec924",
   "metadata": {
    "id": "3abec924",
    "outputId": "183c61d2-781a-4b84-a386-4235eb657486"
   },
   "outputs": [],
   "source": [
    "dfs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4dcffc",
   "metadata": {
    "id": "0b4dcffc",
    "outputId": "472e04e1-3b72-4180-a9a3-31622ae2dfdb"
   },
   "outputs": [],
   "source": [
    "print(dfs_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a26a64",
   "metadata": {
    "id": "d9a26a64",
    "outputId": "4a2d4dd3-3ffd-4f06-83c5-6a98d47285b6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfs_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d08f4d",
   "metadata": {
    "id": "94d08f4d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c4df12",
   "metadata": {
    "id": "52c4df12",
    "outputId": "a8821710-441e-4601-ed78-3f6cc07db93d"
   },
   "outputs": [],
   "source": [
    "print(dfs_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017c07bb",
   "metadata": {
    "id": "017c07bb"
   },
   "source": [
    "### Question 1.c.(i)\n",
    "\n",
    "**The time-domain features which are used in time series classification are namely Minimum, maximum, mean, median, standard deviation, first quartile, and third quartile.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278f3a49",
   "metadata": {
    "id": "278f3a49"
   },
   "source": [
    "### Question 1.c.(ii)\n",
    "minimum, maximum, mean, median, standard\n",
    "deviation, \n",
    "rst quartile, and third quartile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119997ba",
   "metadata": {
    "id": "119997ba"
   },
   "outputs": [],
   "source": [
    "coloums=[]\n",
    "for k in range(1,7):\n",
    "    coloums.append(['min'+str(k),'max'+str(k),r'mean'+str(k),r'median'+str(k),r'std'+str(k),r'1st quart'+str(k),r'3rd quart'+str(k)])\n",
    "fnl_coloumn = [item for sublist in coloums for item in sublist]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc58cec",
   "metadata": {
    "id": "adc58cec"
   },
   "outputs": [],
   "source": [
    "filepath2= r'../data/AReM/bending2'\n",
    "files=glob.glob(os.path.join(filepath2,\"*.csv\"))\n",
    "if (i==\"../data/AReM/bending2\\dataset4.csv\"):\n",
    "        dfs=pd.read_csv(i,sep=' ',skiprows=5,names=[\"# Columns: time\",\"avg_rss12\",\"var_rss12\",\"avg_rss13\",\"var_rss13\",\"avg_rss23\",\"var_rss23\"],index_col=False)  \n",
    "bending1=[]\n",
    "for k in range(1,8):\n",
    "    bending1.append(\"dataset\"+str(k))\n",
    "    \n",
    "filepath_bend= r'../data/AReM/bending1'\n",
    "files=glob.glob(os.path.join(filepath_bend,\"*.csv\"))\n",
    "\n",
    "df_final= pd.DataFrame(columns=[fnl_coloumn])\n",
    "for i in bending1:\n",
    "    df=pd.read_csv(filepath_bend+'/' + str(i)+\".csv\",skiprows=4)\n",
    "    df.drop(columns=['# Columns: time'],axis=1)\n",
    "    temp=[]\n",
    "    for j in df.columns[1:]:\n",
    "        temp.append(df[j].min())\n",
    "        temp.append(df[j].max())\n",
    "        temp.append(df[j].mean())\n",
    "        temp.append(df[j].median())\n",
    "        temp.append(df[j].std())\n",
    "        temp.append(df[j].quantile(0.25))\n",
    "        temp.append(df[j].quantile(0.75))\n",
    "        \n",
    "    df_final.loc[len(df_final)]=temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54103d3",
   "metadata": {
    "id": "f54103d3"
   },
   "outputs": [],
   "source": [
    "bending2=[]\n",
    "for k in range(1,7):\n",
    "    bending2.append(\"dataset\"+str(k))\n",
    "\n",
    "\n",
    "filepath_bend2= r'../data/AReM/bending2'\n",
    "\n",
    "df_final2= pd.DataFrame(columns=[fnl_coloumn])\n",
    "\n",
    "\n",
    "for i in bending2:\n",
    "    temp2=[]\n",
    "    if i == 'dataset4':\n",
    "        path=\"../data/AReM/bending2/\"+str(i)+\".csv\"\n",
    "        df_bend2=pd.read_csv(path,sep=' ',skiprows=5,names=[\"# Columns: time\",\"avg_rss12\",\"var_rss12\",\"avg_rss13\",\"var_rss13\",\"avg_rss23\",\"var_rss23\"],index_col=False)  \n",
    "        for j in df_bend2.columns[1:]:\n",
    "            temp2.append(df_bend2[j].min())\n",
    "            temp2.append(df_bend2[j].max())\n",
    "            temp2.append(df_bend2[j].mean())\n",
    "            temp2.append(df_bend2[j].median())\n",
    "            temp2.append(df_bend2[j].std())\n",
    "            temp2.append(df_bend2[j].quantile(0.25))\n",
    "            temp2.append(df_bend2[j].quantile(0.75))\n",
    "    else:\n",
    "        df_bend2=pd.read_csv(filepath_bend2+'/' + str(i)+\".csv\",skiprows=4)\n",
    "        for j in df_bend2.columns[1:]:\n",
    "            temp2.append(df_bend2[j].min())\n",
    "            temp2.append(df_bend2[j].max())\n",
    "            temp2.append(df_bend2[j].mean())\n",
    "            temp2.append(df_bend2[j].median())\n",
    "            temp2.append(df_bend2[j].std())\n",
    "            temp2.append(df_bend2[j].quantile(0.25))\n",
    "            temp2.append(df_bend2[j].quantile(0.75))\n",
    "    df_final2.loc[len(df_final2)]=temp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0ee131",
   "metadata": {
    "id": "2e0ee131"
   },
   "outputs": [],
   "source": [
    "cycling=[]\n",
    "for k in range(1,16):\n",
    "    cycling.append(\"dataset\"+str(k))\n",
    "filepath_cyc= r'../data/AReM/cycling'\n",
    "files=glob.glob(os.path.join(filepath_cyc,\"*.csv\"))\n",
    "\n",
    "\n",
    "df_final_cyc= pd.DataFrame(columns=[fnl_coloumn])\n",
    "for i in cycling:\n",
    "    df_cyc=pd.read_csv(filepath_cyc+'/' + str(i)+\".csv\",skiprows=4)\n",
    "    temp_cyc=[]\n",
    "    for j in df_cyc.columns[1:]:\n",
    "        temp_cyc.append(df_cyc[j].min())\n",
    "        temp_cyc.append(df_cyc[j].max())\n",
    "        temp_cyc.append(df_cyc[j].mean())\n",
    "        temp_cyc.append(df_cyc[j].median())\n",
    "        temp_cyc.append(df_cyc[j].std())\n",
    "        temp_cyc.append(df_cyc[j].quantile(0.25))\n",
    "        temp_cyc.append(df_cyc[j].quantile(0.75))\n",
    "        \n",
    "    df_final_cyc.loc[len(df_final_cyc)]=temp_cyc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806c98cd",
   "metadata": {
    "id": "806c98cd"
   },
   "outputs": [],
   "source": [
    "lying=[]\n",
    "for k in range(1,16):\n",
    "    lying.append(\"dataset\"+str(k))\n",
    "\n",
    "filepath_ly= r'../data/AReM/lying'\n",
    "files=glob.glob(os.path.join(filepath_ly,\"*.csv\"))\n",
    "\n",
    "df_final_ly= pd.DataFrame(columns=[fnl_coloumn])\n",
    "for i in cycling:\n",
    "    df_ly=pd.read_csv(filepath_ly+'/' + str(i)+\".csv\",skiprows=4)\n",
    "    temp_ly=[]\n",
    "    for j in df_cyc.columns[1:]:\n",
    "        temp_ly.append(df_ly[j].min())\n",
    "        temp_ly.append(df_ly[j].max())\n",
    "        temp_ly.append(df_ly[j].mean())\n",
    "        temp_ly.append(df_ly[j].median())\n",
    "        temp_ly.append(df_ly[j].std())\n",
    "        temp_ly.append(df_ly[j].quantile(0.25))\n",
    "        temp_ly.append(df_ly[j].quantile(0.75))\n",
    "        \n",
    "    df_final_ly.loc[len(df_final_ly)]=temp_ly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec21a9d0",
   "metadata": {
    "id": "ec21a9d0"
   },
   "outputs": [],
   "source": [
    "sitting=[]\n",
    "for k in range(1,16):\n",
    "    sitting.append(\"dataset\"+str(k))\n",
    "\n",
    "filepath_sit= r'../data/AReM/sitting'\n",
    "files=glob.glob(os.path.join(filepath_sit,\"*.csv\"))\n",
    "\n",
    "\n",
    "df_final_sit= pd.DataFrame(columns=[fnl_coloumn])\n",
    "for i in sitting:\n",
    "    df_sit=pd.read_csv(filepath_sit+'/' + str(i)+\".csv\",skiprows=4)\n",
    "    temp_sit=[]\n",
    "    for j in df_sit.columns[1:]:\n",
    "        temp_sit.append(df_sit[j].min())\n",
    "        temp_sit.append(df_sit[j].max())\n",
    "        temp_sit.append(df_sit[j].mean())\n",
    "        temp_sit.append(df_sit[j].median())\n",
    "        temp_sit.append(df_sit[j].std())\n",
    "        temp_sit.append(df_sit[j].quantile(0.25))\n",
    "        temp_sit.append(df_sit[j].quantile(0.75))\n",
    "        \n",
    "    df_final_sit.loc[len(df_final_sit)]=temp_sit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed628901",
   "metadata": {
    "id": "ed628901"
   },
   "outputs": [],
   "source": [
    "standing=[]\n",
    "for k in range(1,16):\n",
    "    standing.append(\"dataset\"+str(k))\n",
    "\n",
    "filepath_stan= r'../data/AReM/standing'\n",
    "files=glob.glob(os.path.join(filepath_stan,\"*.csv\"))\n",
    "\n",
    "\n",
    "df_final_stan= pd.DataFrame(columns=[fnl_coloumn])\n",
    "for i in standing:\n",
    "    df_stan=pd.read_csv(filepath_stan+'/' + str(i)+\".csv\",skiprows=4)\n",
    "    temp_stan=[]\n",
    "    for j in df_stan.columns[1:]:\n",
    "        temp_stan.append(df_stan[j].min())\n",
    "        temp_stan.append(df_stan[j].max())\n",
    "        temp_stan.append(df_stan[j].mean())\n",
    "        temp_stan.append(df_stan[j].median())\n",
    "        temp_stan.append(df_stan[j].std())\n",
    "        temp_stan.append(df_stan[j].quantile(0.25))\n",
    "        temp_stan.append(df_stan[j].quantile(0.75))\n",
    "        \n",
    "    df_final_stan.loc[len(df_final_stan)]=temp_stan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38500329",
   "metadata": {
    "id": "38500329"
   },
   "outputs": [],
   "source": [
    "walking=[]\n",
    "for k in range(1,16):\n",
    "    walking.append(\"dataset\"+str(k))\n",
    "\n",
    "filepath_walk= r'../data/AReM/walking'\n",
    "files=glob.glob(os.path.join(filepath_walk,\"*.csv\"))\n",
    "\n",
    "\n",
    "df_final_walk= pd.DataFrame(columns=[fnl_coloumn])\n",
    "for i in walking:\n",
    "    df_walk=pd.read_csv(filepath_walk+'/' + str(i)+\".csv\",skiprows=4)\n",
    "    temp_walk=[]\n",
    "    for j in df_walk.columns[1:]:\n",
    "        temp_walk.append(df_walk[j].min())\n",
    "        temp_walk.append(df_walk[j].max())\n",
    "        temp_walk.append(df_walk[j].mean())\n",
    "        temp_walk.append(df_walk[j].median())\n",
    "        temp_walk.append(df_walk[j].std())\n",
    "        temp_walk.append(df_walk[j].quantile(0.25))\n",
    "        temp_walk.append(df_walk[j].quantile(0.75))\n",
    "        \n",
    "    df_final_walk.loc[len(df_final_walk)]=temp_walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad7c77b",
   "metadata": {
    "id": "aad7c77b",
    "outputId": "327a4834-6aaa-47ba-fe47-63010103dde6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frames=[df_final,df_final2,df_final_cyc,df_final_ly,df_final_sit,df_final_stan,df_final_walk]\n",
    "fnl_df= pd.concat(frames,ignore_index=True)\n",
    "fnl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3c0d98",
   "metadata": {
    "id": "0d3c0d98"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3225c69a",
   "metadata": {
    "id": "3225c69a"
   },
   "source": [
    "### Question 1.c.(iii)\n",
    "\n",
    "Estimate the standard deviation of each of the time-domain features you\n",
    "extracted from the data. Then, use Python's bootstrapped or any other\n",
    "method to build a 90% bootsrap con\n",
    "dence interval for the standard deviation\n",
    "of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081ecd6f",
   "metadata": {
    "id": "081ecd6f",
    "outputId": "48dc8d7f-b925-4497-f1ef-40c2fad1d3f6"
   },
   "outputs": [],
   "source": [
    "fnl_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fab0246",
   "metadata": {
    "id": "2fab0246",
    "outputId": "ea03a1bb-23e0-463b-cbe8-42418cd16b47",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fnl_df.describe().loc[\"std\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70858c16",
   "metadata": {
    "id": "70858c16",
    "outputId": "d4110b23-2859-4639-d862-e2bc24e7f677"
   },
   "outputs": [],
   "source": [
    "conf_interval=pd.DataFrame(columns=[\"The features\",\"90% confidence interval\"])\n",
    "conf_intrvl=[]\n",
    "conf_interval[\"The features\"] =fnl_coloumn \n",
    "for i in range (0,len(fnl_df.columns)):\n",
    "  # samples must be in a sequence\n",
    "    res = bootstrap((fnl_df.iloc[:,i],), np.std, confidence_level=0.9,random_state=20, method='percentile')\n",
    "    conf_intrvl.append(res.confidence_interval)\n",
    "# print(conf_intrvl)\n",
    "conf_interval[\"90% confidence interval\"]=pd.Series(conf_intrvl)\n",
    "conf_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8433237d",
   "metadata": {
    "id": "8433237d"
   },
   "source": [
    "### Question i.c.(iv)\n",
    "Use your judgement to select the three most important time-domain features\n",
    "(one option may be min, mean, and max)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240ca200",
   "metadata": {
    "id": "240ca200"
   },
   "source": [
    "**According to me the most important features are namely Standard Deviation, median, and mean.\n",
    "the most important features to seem like mean, median and standard deviation.**\n",
    "\n",
    "**1) Standard Deviaiton because a) its a key features in determining the expansion of the curve and  how it's shaped along the values of features. b) In our dataset, the range of confidence interval is the lowest for standard deviation and hence its an important features.**\n",
    "\n",
    "**2) Median because a) its range of confidence interval is also among the smallest b) median gives how skewed the distribution is,and represents the half of dataset and therefore gives a better perspective of the data. .**\n",
    "\n",
    "**3) Mean because a)it  gives the information about the position of peak of the distribution and the type of distribution b) In our dataset its range for confidence interval is among the smallest ones.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee76fbd2",
   "metadata": {
    "id": "ee76fbd2"
   },
   "source": [
    "### Question 2\n",
    "ISLR 3.7.4\n",
    "\n",
    "I collect a set of data (n = 100 observations) containing a single\n",
    "predictor and a quantitative response. I then fit a linear regression\n",
    "model to the data, as well as a separate cubic regression, i.e. Y =\n",
    "β0 + β1X + β2X2 + β3X3 + ϵ.\n",
    "\n",
    "(a) Suppose that the true relationship between X and Y is linear,\n",
    "i.e. Y = β0 + β1X + ϵ. Consider the training residual sum of\n",
    "squares (RSS) for the linear regression, and also the training\n",
    "RSS for the cubic regression. Would we expect one to be lower\n",
    "than the other, would we expect them to be the same, or is there\n",
    "not enough information to tell? Justify your answer.\n",
    "\n",
    "(b) Answer (a) using test rather than training RSS.\n",
    "\n",
    "(c) Suppose that the true relationship between X and Y is not linear,\n",
    "but we don’t know how far it is from linear. Consider the training\n",
    "RSS for the linear regression, and also the training RSS for the\n",
    "cubic regression. Would we expect one to be lower than the\n",
    "other, would we expect them to be the same, or is there not\n",
    "enough information to tell? Justify your answer.\n",
    "\n",
    "(d) Answer (c) using test rather than training RSS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27aaa2d8",
   "metadata": {
    "id": "27aaa2d8"
   },
   "source": [
    "![WhatsApp%20Image%202022-10-07%20at%204.27.54%20PM.jpeg](attachment:WhatsApp%20Image%202022-10-07%20at%204.27.54%20PM.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4b8858",
   "metadata": {
    "id": "2d4b8858"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
